{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffa32dc",
   "metadata": {},
   "source": [
    "# Algorithmic Trading Pipeline\n",
    "# Complete Workflow: Data â†’ Model â†’ Strategy â†’ Backtest â†’ Analysis\n",
    "\n",
    "This notebook demonstrates the full pipeline for:\n",
    "1. Loading/creating data catalog\n",
    "2. Model hyperparameter optimization\n",
    "3. Strategy hyperparameter optimization\n",
    "4. Final backtest execution\n",
    "5. Performance analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd62c7",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 0: Setup & Configuration\n",
    "Load dependencies and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93be3c18",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (hparam_tuner.py, line 625)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[36m  \u001b[39m\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom engine.hparam_tuner import OptunaHparamsTuner\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\engine\\hparam_tuner.py:625\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfor metric_name, metric_value in metrics.items():\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Use to debugging\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Core imports\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import logging\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "\n",
    "# Nautilus Trader\n",
    "from nautilus_trader.model.objects import Currency\n",
    "from nautilus_trader.core.nautilus_pyo3 import CurrencyType\n",
    "from nautilus_trader.persistence.catalog import ParquetDataCatalog\n",
    "from nautilus_trader.model.data import TradeTick\n",
    "\n",
    "# Project modules\n",
    "from engine.databento_loader import DatabentoTickLoader\n",
    "from engine.hparam_tuner import OptunaHparamsTuner\n",
    "from engine.performance_plots import (\n",
    "    get_frequency_params, align_series,\n",
    "    plot_balance_breakdown, plot_cumulative_returns,\n",
    "    plot_rolling_sharpe, plot_underwater,\n",
    "    plot_active_returns, plot_portfolio_allocation\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "client = MlflowClient(tracking_uri=\"file:logs/mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dab57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sapiens configuration\n",
    "sapiens_cfg_path = Path(\"configs/sapiens_config.yaml\")\n",
    "sapiens_cfg = yaml.safe_load(sapiens_cfg_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Load strategy and model configs\n",
    "strategy_name = sapiens_cfg[\"SAPIENS_STRATEGY\"]['strategy_name']\n",
    "strategy_cfg_path = Path(f\"strategies/{strategy_name}/strategy_config.yaml\")\n",
    "strategy_cfg = yaml.safe_load(strategy_cfg_path.read_text(encoding=\"utf-8\"))[\"STRATEGY\"]\n",
    "\n",
    "model_name = sapiens_cfg[\"SAPIENS_MODEL\"]['model_name']\n",
    "model_cfg_path = Path(f\"models/{model_name}/model_config.yaml\")\n",
    "model_cfg = yaml.safe_load(model_cfg_path.read_text(encoding=\"utf-8\"))[\"MODEL\"]\n",
    "\n",
    "\n",
    "# Setup directories\n",
    "logs_dir = Path(sapiens_cfg[\"logs_dir\"])\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Model: {model_cfg['PARAMS']['model_name']}\")\n",
    "print(f\"Strategy: {strategy_cfg['PARAMS']['strategy_name']}\")\n",
    "print(f\"Backtest period: {sapiens_cfg['backtest_start']} to {sapiens_cfg['backtest_end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619ef4b",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Data Catalog and HyperParameter Tuner setup\n",
    "Load or create Nautilus Trader data catalog from Databento tick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "FORCE_RELOAD_CATALOG = False  # Set to True to rebuild catalog\n",
    "CATALOG_PATH = None  # Set custom path or None for default\n",
    "\n",
    "# Initialize loader\n",
    "logger.info(\"Initializing Databento loader...\")\n",
    "loader = DatabentoTickLoader(\n",
    "    cfg=strategy_cfg[\"PARAMS\"],\n",
    "    venue_name=strategy_cfg[\"PARAMS\"][\"venue_name\"]\n",
    ")\n",
    "\n",
    "# Determine catalog path\n",
    "catalog_path = Path(CATALOG_PATH) if CATALOG_PATH else loader.catalog_path\n",
    "\n",
    "# Load or create catalog\n",
    "if not FORCE_RELOAD_CATALOG and loader.catalog_exists(catalog_path):\n",
    "    logger.info(f\"ðŸ“‚ Reusing existing catalog at: {catalog_path}\")\n",
    "    catalog = ParquetDataCatalog(path=str(catalog_path))\n",
    "else:\n",
    "    logger.info(f\"ðŸ”„ Loading Databento ticks to catalog at: {catalog_path}\")\n",
    "    if FORCE_RELOAD_CATALOG:\n",
    "        logger.info(\"Force reload enabled - rebuilding catalog\")\n",
    "    \n",
    "    # Load with progress bar and memory management\n",
    "    catalog = loader.load_to_catalog(\n",
    "        catalog_path=catalog_path,\n",
    "    )\n",
    "\n",
    "# Add catalog path to config\n",
    "strategy_cfg[\"PARAMS\"][\"catalog_path\"] = str(catalog_path)\n",
    "\n",
    "# Verify catalog\n",
    "#instruments = catalog.instruments(instrument_type=TradeTick)  # takes too long on laptop. Use loader class instruments property instead\n",
    "instruments = set(inst.id.value for inst in catalog.instruments())\n",
    "print(f\"\\nâœ… Catalog ready: {catalog.list_data_types()} data loaded\")\n",
    "print(f\"Universe: {[str(symbol) for symbol in instruments]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca069536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparameter tuner\n",
    "tuner = OptunaHparamsTuner(\n",
    "    sapiens_config=sapiens_cfg,\n",
    "    catalog=catalog,\n",
    "    model_config=model_cfg,\n",
    "    strategy_config=strategy_cfg,\n",
    "    run_dir=logs_dir\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter tuner initialized\")\n",
    "print(f\"Model trials: {sapiens_cfg['SAPIENS_MODEL']['optimization']['n_trials']}\")\n",
    "print(f\"Strategy trials: {sapiens_cfg['SAPIENS_STRATEGY']['optimization']['n_trials']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d7615",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Model Hyperparameter Optimization\n",
    "Optimize model hyperparameters using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82825222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model hyperparameter optimization\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"ðŸ”¬ STAGE 2: MODEL HYPERPARAMETER OPTIMIZATION\")\n",
    "logger.info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "model_results = tuner.optimize_model()\n",
    "\n",
    "print(\"\\nâœ… Model optimization complete!\")\n",
    "print(f\"Best model path: {model_results['model_path']}\")\n",
    "print(f\"MLflow run ID: {model_results['mlflow_run_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a2a6c",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Strategy Hyperparameter Optimization\n",
    "Optimize strategy hyperparameters using best model from Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to debugging\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# Run strategy hyperparameter optimization\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"ðŸ“Š STAGE 3: STRATEGY HYPERPARAMETER OPTIMIZATION\")\n",
    "logger.info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "model_name = model_cfg['PARAMS']['model_name']\n",
    "strategy_results = tuner.optimize_strategy(model_name=model_name)\n",
    "\n",
    "print(\"\\nâœ… Strategy optimization complete!\")\n",
    "print(f\"Best hyperparameters: {strategy_results['hparams']}\")\n",
    "print(f\"\\nBest metrics:\")\n",
    "for metric, value in strategy_results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "print(f\"\\nMLflow run ID: {strategy_results['mlflow_run_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491401ff",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Final Backtest\n",
    "Run final backtest on full period with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimization context\n",
    "run = client.get_run(strategy_results[\"mlflow_run_id\"])\n",
    "optimization_id = run.data.tags.get(\"optimization_id\", \"\")\n",
    "\n",
    "# Define backtest period\n",
    "backtest_start = sapiens_cfg[\"backtest_start\"]\n",
    "backtest_end = sapiens_cfg[\"backtest_end\"]\n",
    "\n",
    "print(f\"Running final backtest: {backtest_start} to {backtest_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute final backtest\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"ðŸš€ STAGE 4: FINAL BACKTEST\")\n",
    "logger.info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "final_metrics, final_time_series = tuner.run_final_backtest(\n",
    "    backtest_start=backtest_start,\n",
    "    backtest_end=backtest_end,\n",
    "    strategy_hpo_run_id=strategy_results[\"mlflow_run_id\"],\n",
    "    optimization_id=optimization_id\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Final backtest complete!\")\n",
    "print(\"\\nFinal Performance Metrics:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in sorted(final_metrics.items()):\n",
    "    print(f\"{metric:.<40} {value:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dccb9a",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: Performance Analysis\n",
    "Detailed analysis and visualization of backtest results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from MLflow\n",
    "\n",
    "# Get the most recent backtest run\n",
    "exp = client.get_experiment_by_name(\"Backtests\")\n",
    "if not exp:\n",
    "    exp_id = client.create_experiment(\"Backtests\")\n",
    "else:\n",
    "    exp_id = exp.experiment_id\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[exp_id],\n",
    "    order_by=[\"start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "backtest_run = runs[0]\n",
    "backtest_run_id = backtest_run.info.run_id\n",
    "\n",
    "print(f\"Loading backtest run: {backtest_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6cbe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load account and positions data\n",
    "acc_path = client.download_artifacts(run_id=backtest_run_id, path=\"account_report.json\")\n",
    "pos_path = client.download_artifacts(run_id=backtest_run_id, path=\"positions_report.json\")\n",
    "\n",
    "account_df = pd.read_csv(acc_path, index_col=0, parse_dates=True)\n",
    "positions_df = pd.read_csv(pos_path, index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"Account snapshots: {len(account_df)}\")\n",
    "print(f\"Position records: {len(positions_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448314cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate returns\n",
    "freq = strategy_cfg['PARAMS']['freq']\n",
    "freq_params = get_frequency_params(freq)\n",
    "\n",
    "# Extract portfolio values\n",
    "currency_code = strategy_cfg['PARAMS']['currency']\n",
    "portfolio_values = account_df[account_df['currency'] == currency_code]['total']\n",
    "portfolio_values = portfolio_values.resample(freq_params['resample_freq']).last().ffill()\n",
    "strategy_ret = portfolio_values.pct_change().fillna(0)\n",
    "\n",
    "# Load benchmark and risk-free data\n",
    "data_dict = tuner.get_ohlcv_data_from_catalog(\n",
    "    frequency=freq,\n",
    "    start=pd.Timestamp(backtest_start),\n",
    "    end=pd.Timestamp(backtest_end),\n",
    "    instrument_ids=[strategy_cfg['PARAMS']['benchmark_ticker'],\n",
    "                    strategy_cfg['PARAMS']['risk_free_ticker']]\n",
    ")\n",
    "benchmark_ret = data_dict[strategy_cfg['PARAMS']['benchmark_ticker']]['close'].pct_change()\n",
    "rf_ret = data_dict[strategy_cfg['PARAMS']['risk_free_ticker']]['close'].pct_change()\n",
    "\n",
    "# Align series\n",
    "strategy_ret, benchmark_ret, rf_ret = align_series(\n",
    "    strategy_ret, benchmark_ret, rf_ret, freq_params['resample_freq']\n",
    ")\n",
    "\n",
    "print(f\"Returns calculated for {len(strategy_ret)} periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d53b1",
   "metadata": {},
   "source": [
    "### 5.1: Balance Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_balance_breakdown(\n",
    "    account_df=account_df,\n",
    "    resample_freq=freq_params['resample_freq']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7932ff",
   "metadata": {},
   "source": [
    "### 5.2: Cumulative Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751366d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_cumulative_returns(\n",
    "    strategy_ret=strategy_ret,\n",
    "    benchmark_ret=benchmark_ret\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949c63f",
   "metadata": {},
   "source": [
    "### 5.3: Rolling Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = max(10, int(freq_params['periods_per_year'] / 12))\n",
    "fig = plot_rolling_sharpe(\n",
    "    strategy_ret=strategy_ret,\n",
    "    benchmark_ret=benchmark_ret,\n",
    "    rf_ret=rf_ret,\n",
    "    window=window,\n",
    "    annualization_factor=freq_params['annualization_factor']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fd1f8",
   "metadata": {},
   "source": [
    "### 5.4: Drawdown Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526edea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_underwater(strategy_ret=strategy_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c1b31",
   "metadata": {},
   "source": [
    "### 5.5: Active Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba177e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_active_returns(\n",
    "    strategy_ret=strategy_ret,\n",
    "    benchmark_ret=benchmark_ret,\n",
    "    freq=freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fda249",
   "metadata": {},
   "source": [
    "### 5.6: Portfolio Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e71db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig = plot_portfolio_allocation(\n",
    "    positions_df=positions_df,\n",
    "    resample_freq=freq_params['resample_freq']\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660082b",
   "metadata": {},
   "source": [
    "### 5.7: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "total_return = (1 + strategy_ret).prod() - 1\n",
    "annualized_return = (1 + total_return) ** (freq_params['periods_per_year'] / len(strategy_ret)) - 1\n",
    "annualized_vol = strategy_ret.std() * freq_params['annualization_factor']\n",
    "sharpe_ratio = (strategy_ret.mean() - rf_ret.mean()) / strategy_ret.std() * freq_params['annualization_factor']\n",
    "\n",
    "# Drawdown\n",
    "cumulative = (1 + strategy_ret).cumprod()\n",
    "running_max = cumulative.expanding().max()\n",
    "drawdown = (cumulative - running_max) / running_max\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "# Win rate\n",
    "winning_periods = (strategy_ret > 0).sum()\n",
    "win_rate = winning_periods / len(strategy_ret)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Return (%)',\n",
    "        'Annualized Return (%)',\n",
    "        'Annualized Volatility (%)',\n",
    "        'Sharpe Ratio',\n",
    "        'Max Drawdown (%)',\n",
    "        'Win Rate (%)',\n",
    "        'Number of Periods'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{total_return * 100:.2f}\",\n",
    "        f\"{annualized_return * 100:.2f}\",\n",
    "        f\"{annualized_vol * 100:.2f}\",\n",
    "        f\"{sharpe_ratio:.2f}\",\n",
    "        f\"{max_drawdown * 100:.2f}\",\n",
    "        f\"{win_rate * 100:.2f}\",\n",
    "        len(strategy_ret)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773ed98",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison Matrix: All Models Ã— Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HPO results matrix\n",
    "hpo_matrix = tuner.get_strategy_hpo_matrix(metric=\"total_pnl_pct\")\n",
    "print(\"\\nStrategy HPO Results Matrix (total_pnl_pct):\")\n",
    "print(hpo_matrix)\n",
    "\n",
    "# Generate final backtest results matrix\n",
    "backtest_matrix = tuner.get_final_backtest_matrix(metric=\"sharpe_ratio\")\n",
    "print(\"\\nFinal Backtest Results Matrix (sharpe_ratio):\")\n",
    "print(backtest_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1faa12",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Complete âœ…\n",
    "\n",
    "**Next Steps:**\n",
    "- Review MLflow UI: `mlflow ui --backend-store-uri logs/mlflow`\n",
    "- Explore experiment tracking and compare runs\n",
    "- Adjust hyperparameters in `configs/config.yaml` and rerun\n",
    "- Export results for production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
