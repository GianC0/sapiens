{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffa32dc",
   "metadata": {},
   "source": [
    "# Algorithmic Trading Pipeline\n",
    "# Complete Workflow: Data â†’ Model â†’ Strategy â†’ Backtest â†’ Analysis\n",
    "\n",
    "This notebook demonstrates the full pipeline for:\n",
    "1. Loading/creating data catalog\n",
    "2. Model hyperparameter optimization\n",
    "3. Strategy hyperparameter optimization\n",
    "4. Final backtest execution\n",
    "5. Performance analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd62c7",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 0: Setup & Configuration\n",
    "Load dependencies and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93be3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to debugging\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Core imports\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import logging\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "\n",
    "# Nautilus Trader\n",
    "from nautilus_trader.model.objects import Currency\n",
    "from nautilus_trader.core.nautilus_pyo3 import CurrencyType\n",
    "from nautilus_trader.persistence.catalog import ParquetDataCatalog\n",
    "from nautilus_trader.model.data import TradeTick\n",
    "\n",
    "# Project modules\n",
    "from algos.engine.databento_loader import DatabentoTickLoader\n",
    "from algos.engine.hparam_tuner import OptunaHparamsTuner\n",
    "from algos.engine.performance_plots import (\n",
    "    get_frequency_params, align_series,\n",
    "    plot_balance_breakdown, plot_cumulative_returns,\n",
    "    plot_rolling_sharpe, plot_underwater,\n",
    "    plot_active_returns, plot_portfolio_allocation\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69dab57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n",
      "Model: UMIModel\n",
      "Strategy: TopKStrategy\n",
      "Backtest period: 2024-06-01 to 2024-07-01\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "cfg_path = Path(\"configs/config.yaml\")\n",
    "cfg = yaml.safe_load(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Setup currency\n",
    "currency_code = cfg[\"STRATEGY\"][\"PARAMS\"][\"currency\"]\n",
    "if currency_code == \"USD\":\n",
    "    cfg[\"STRATEGY\"][\"PARAMS\"][\"currency\"] = Currency(\n",
    "        code='USD', precision=3, iso4217=840,\n",
    "        name='United States dollar', currency_type=CurrencyType.FIAT\n",
    "    )\n",
    "elif currency_code == \"EUR\":\n",
    "    cfg[\"STRATEGY\"][\"PARAMS\"][\"currency\"] = Currency(\n",
    "        code='EUR', precision=3, iso4217=978,\n",
    "        name='Euro', currency_type=CurrencyType.FIAT\n",
    "    )\n",
    "\n",
    "# Setup directories\n",
    "logs_dir = Path(\"logs/\")\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Model: {cfg['MODEL']['PARAMS']['model_name']}\")\n",
    "print(f\"Strategy: {cfg['STRATEGY']['PARAMS']['strategy_name']}\")\n",
    "print(f\"Backtest period: {cfg['STRATEGY']['PARAMS']['backtest_start']} to {cfg['STRATEGY']['PARAMS']['backtest_end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619ef4b",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 1: Data Catalog and HyperParameter Tuner setup\n",
    "Load or create Nautilus Trader data catalog from Databento tick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e3fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 13:26:14,687 - __main__ - INFO - Initializing Databento loader...\n",
      "2025-10-26 13:26:14,705 - __main__ - INFO - ðŸ“‚ Reusing existing catalog at: C:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\data\\nautilus_catalog\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Catalog ready: ['trade_tick'] data loaded\n",
      "Universe: ['AAPL', 'GOOGL', 'MSFT', 'SGOV', 'SPY']\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "FORCE_RELOAD_CATALOG = False  # Set to True to rebuild catalog\n",
    "CATALOG_PATH = None  # Set custom path or None for default\n",
    "\n",
    "# Initialize loader\n",
    "logger.info(\"Initializing Databento loader...\")\n",
    "loader = DatabentoTickLoader(\n",
    "    cfg=cfg[\"STRATEGY\"][\"PARAMS\"],\n",
    "    venue_name=cfg[\"STRATEGY\"][\"PARAMS\"][\"venue_name\"]\n",
    ")\n",
    "\n",
    "# Determine catalog path\n",
    "catalog_path = Path(CATALOG_PATH) if CATALOG_PATH else loader.catalog_path\n",
    "\n",
    "# Load or create catalog\n",
    "if not FORCE_RELOAD_CATALOG and loader.catalog_exists(catalog_path):\n",
    "    logger.info(f\"ðŸ“‚ Reusing existing catalog at: {catalog_path}\")\n",
    "    catalog = ParquetDataCatalog(path=str(catalog_path))\n",
    "else:\n",
    "    logger.info(f\"ðŸ”„ Loading Databento ticks to catalog at: {catalog_path}\")\n",
    "    if FORCE_RELOAD_CATALOG:\n",
    "        logger.info(\"Force reload enabled - rebuilding catalog\")\n",
    "    \n",
    "    # Load with progress bar and memory management\n",
    "    catalog = loader.load_to_catalog(\n",
    "        catalog_path=catalog_path,\n",
    "    )\n",
    "\n",
    "# Add catalog path to config\n",
    "cfg[\"STRATEGY\"][\"PARAMS\"][\"catalog_path\"] = str(catalog_path)\n",
    "\n",
    "# Verify catalog\n",
    "#instruments = catalog.instruments(instrument_type=TradeTick)  # takes too long on laptop. Use loader class instruments property instead\n",
    "instruments = loader.instruments\n",
    "instrument_ids = [inst.id for inst in loader.instruments]\n",
    "print(f\"\\nâœ… Catalog ready: {catalog.list_data_types()} data loaded\")\n",
    "print(f\"Universe: {[str(inst.id.symbol) for inst in instruments[:10]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca069536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuner initialized\n",
      "Model trials: 2\n",
      "Strategy trials: 2\n"
     ]
    }
   ],
   "source": [
    "# Initialize hyperparameter tuner\n",
    "tuner = OptunaHparamsTuner(\n",
    "    cfg=cfg,\n",
    "    catalog=catalog,\n",
    "    run_dir=logs_dir,\n",
    "    seed=2025\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter tuner initialized\")\n",
    "print(f\"Model trials: {cfg['MODEL']['PARAMS']['n_trials']}\")\n",
    "print(f\"Strategy trials: {cfg['STRATEGY']['PARAMS']['n_trials']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d7615",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: Model Hyperparameter Optimization\n",
    "Optimize model hyperparameters using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82825222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 13:26:14,877 - __main__ - INFO - \n",
      "======================================================================\n",
      "2025-10-26 13:26:14,878 - __main__ - INFO - ðŸ”¬ STAGE 2: MODEL HYPERPARAMETER OPTIMIZATION\n",
      "2025-10-26 13:26:14,878 - __main__ - INFO - ======================================================================\n",
      "\n",
      "[I 2025-10-26 13:26:16,156] Using an existing study with name 'UMIModel' instead of creating a new one.\n",
      "2025-10-26 13:26:16,178 - algos.engine.hparam_tuner - INFO - Running 2 model trials...\n",
      "2025-10-26 13:26:17,210 - algos.engine.hparam_tuner - INFO - AAPL.SIM: 534,067 total ticks â†’ 513,372 in market hours (20,695 pre/after-hours excluded, 3.9%)\n",
      "2025-10-26 13:26:17,713 - algos.engine.hparam_tuner - INFO - GOOGL.SIM: 303,099 total ticks â†’ 293,277 in market hours (9,822 pre/after-hours excluded, 3.2%)\n",
      "2025-10-26 13:26:18,244 - algos.engine.hparam_tuner - INFO - MSFT.SIM: 437,805 total ticks â†’ 424,029 in market hours (13,776 pre/after-hours excluded, 3.1%)\n",
      "2025-10-26 13:26:18,290 - algos.engine.hparam_tuner - INFO - SGOV.SIM: 2,272 total ticks â†’ 1,998 in market hours (274 pre/after-hours excluded, 12.1%)\n",
      "2025-10-26 13:26:18,703 - algos.engine.hparam_tuner - INFO - SPY.SIM: 295,546 total ticks â†’ 279,734 in market hours (15,812 pre/after-hours excluded, 5.4%)\n",
      "2025-10-26 13:26:18,717 - algos.engine.hparam_tuner - INFO - âœ… All 5 instruments aligned: 52 bars\n",
      "2025-10-26 13:26:18,974 - models.UMIModel.UMIModel - INFO - [UMIModel] Initializing model...\n",
      "c:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      "2025-10-26 13:26:19,019 - models.UMIModel.UMIModel - INFO - [initialize] Training for 20 epochs...\n",
      "2025-10-26 13:26:19,020 - models.UMIModel.UMIModel - INFO - [_train] Starting training with mode: Sequential\n",
      "2025-10-26 13:26:20,277 - models.UMIModel.UMIModel - INFO - [sequential] Phase 1: Stage-1 training with early stopping\n",
      "2025-10-26 13:26:20,402 - models.UMIModel.UMIModel - INFO - Stage1 Epoch   0 | train 159806.48047 | val 1.01000 | best 1.01000 | patience 0\n",
      "2025-10-26 13:26:20,476 - models.UMIModel.UMIModel - INFO - Stage1 Epoch   1 | train 159500.96094 | val 1.01000 | best 1.01000 | patience 1\n",
      "2025-10-26 13:26:20,613 - models.UMIModel.UMIModel - INFO - [sequential] Early stopping after 3 epochs without improvement\n",
      "2025-10-26 13:26:20,614 - models.UMIModel.UMIModel - INFO - [sequential] Restoring best Stage-1 weights\n",
      "2025-10-26 13:26:20,614 - models.UMIModel.UMIModel - INFO - [sequential] Phase 2: Stage-2 training for 20 epochs\n",
      "2025-10-26 13:26:20,746 - models.UMIModel.UMIModel - INFO - Stage2 Epoch   3 | train 159501.96703 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:20,852 - models.UMIModel.UMIModel - INFO - Stage2 Epoch   4 | train 159501.96703 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:21,092 - models.UMIModel.UMIModel - INFO - Stage2 Epoch   6 | train 159501.96312 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:21,321 - models.UMIModel.UMIModel - INFO - Stage2 Epoch   8 | train 159501.96703 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:21,549 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  10 | train 159501.95922 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:21,775 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  12 | train 159501.96703 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:22,010 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  14 | train 159501.95922 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:22,243 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  16 | train 159501.95922 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:22,478 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  18 | train 159501.96703 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:22,718 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  20 | train 159501.95922 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:22,949 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  22 | train 159501.96312 | val 1.01000 | best 1.01000\n",
      "2025-10-26 13:26:22,951 - models.UMIModel.UMIModel - INFO - [_train] Training completed. Best validation loss: 1.010000\n",
      "2025-10-26 13:26:22,982 - models.UMIModel.UMIModel - INFO - [initialize] Complete. Best validation: 1.010000\n",
      "2025-10-26 13:26:24,730 - algos.engine.hparam_tuner - INFO -   Trial 6: loss = 1.010000\n",
      "[I 2025-10-26 13:26:24,745] Trial 6 finished with value: 1.0099999904632568 and parameters: {'train_offset': '5B', 'window_len': 15, 'lambda_ic': 0, 'lambda_sync': 0.5, 'lambda_rankic': 0.05, 'temperature': 0.09391191739944524, 'sync_thr': 0.590034501423391, 'lr_stage1': 0.0002644609945389542, 'lr_stage1_ft': 0.0013557518238705938, 'lr_stage2': 0.0009442039690998861, 'weight_decay': 0.0}. Best is trial 4 with value: 0.14732937514781952.\n",
      "2025-10-26 13:26:25,619 - algos.engine.hparam_tuner - INFO - AAPL.SIM: 534,067 total ticks â†’ 513,372 in market hours (20,695 pre/after-hours excluded, 3.9%)\n",
      "2025-10-26 13:26:26,097 - algos.engine.hparam_tuner - INFO - GOOGL.SIM: 303,099 total ticks â†’ 293,277 in market hours (9,822 pre/after-hours excluded, 3.2%)\n",
      "2025-10-26 13:26:26,610 - algos.engine.hparam_tuner - INFO - MSFT.SIM: 437,805 total ticks â†’ 424,029 in market hours (13,776 pre/after-hours excluded, 3.1%)\n",
      "2025-10-26 13:26:26,655 - algos.engine.hparam_tuner - INFO - SGOV.SIM: 2,272 total ticks â†’ 1,998 in market hours (274 pre/after-hours excluded, 12.1%)\n",
      "2025-10-26 13:26:27,078 - algos.engine.hparam_tuner - INFO - SPY.SIM: 295,546 total ticks â†’ 279,734 in market hours (15,812 pre/after-hours excluded, 5.4%)\n",
      "2025-10-26 13:26:27,093 - algos.engine.hparam_tuner - INFO - âœ… All 5 instruments aligned: 52 bars\n",
      "2025-10-26 13:26:27,194 - models.UMIModel.UMIModel - INFO - [UMIModel] Initializing model...\n",
      "c:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      "2025-10-26 13:26:27,197 - models.UMIModel.UMIModel - INFO - [initialize] Training for 20 epochs...\n",
      "2025-10-26 13:26:27,198 - models.UMIModel.UMIModel - INFO - [_train] Starting training with mode: Sequential\n",
      "2025-10-26 13:26:27,199 - models.UMIModel.UMIModel - INFO - [sequential] Phase 1: Stage-1 training with early stopping\n",
      "2025-10-26 13:26:27,252 - models.UMIModel.UMIModel - INFO - Stage1 Epoch   0 | train 296139.02344 | val 1594464256.00000 | best 1594464256.00000 | patience 0\n",
      "2025-10-26 13:26:27,307 - models.UMIModel.UMIModel - INFO - Stage1 Epoch   1 | train 294304.16406 | val 1594338560.00000 | best 1594464256.00000 | patience 1\n",
      "2025-10-26 13:26:27,415 - models.UMIModel.UMIModel - INFO - [sequential] Early stopping after 3 epochs without improvement\n",
      "2025-10-26 13:26:27,416 - models.UMIModel.UMIModel - INFO - [sequential] Restoring best Stage-1 weights\n",
      "2025-10-26 13:26:27,416 - models.UMIModel.UMIModel - INFO - [sequential] Phase 2: Stage-2 training for 20 epochs\n",
      "2025-10-26 13:26:27,520 - models.UMIModel.UMIModel - INFO - Stage2 Epoch   3 | train 2022007840.14062 | val 1322447744.00000 | best 1322447744.00000\n",
      "2025-10-26 13:26:27,619 - models.UMIModel.UMIModel - INFO - Stage2 Epoch   4 | train 1644193184.16406 | val 1079631488.00000 | best 1079631488.00000\n",
      "2025-10-26 13:26:27,819 - models.UMIModel.UMIModel - INFO - Stage2 Epoch   6 | train 1018636256.14844 | val 675671488.00000 | best 675671488.00000\n",
      "2025-10-26 13:26:28,019 - models.UMIModel.UMIModel - INFO - Stage2 Epoch   8 | train 563352928.14844 | val 376614912.00000 | best 376614912.00000\n",
      "2025-10-26 13:26:28,230 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  10 | train 266488336.14844 | val 176032400.00000 | best 176032400.00000\n",
      "2025-10-26 13:26:28,433 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  12 | train 101402456.16406 | val 60797120.00000 | best 60797120.00000\n",
      "2025-10-26 13:26:28,630 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  14 | train 25090826.16406 | val 11247116.00000 | best 11247116.00000\n",
      "2025-10-26 13:26:28,834 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  16 | train 1464638.17188 | val 18652.78516 | best 18652.78516\n",
      "2025-10-26 13:26:29,030 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  18 | train 294305.20406 | val 34.04860 | best 34.04860\n",
      "2025-10-26 13:26:29,230 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  20 | train 294305.18063 | val 29.46693 | best 29.46693\n",
      "2025-10-26 13:26:29,447 - models.UMIModel.UMIModel - INFO - Stage2 Epoch  22 | train 294305.18063 | val 25.94621 | best 25.94621\n",
      "2025-10-26 13:26:29,448 - models.UMIModel.UMIModel - INFO - [_train] Training completed. Best validation loss: 25.946207\n",
      "2025-10-26 13:26:29,455 - models.UMIModel.UMIModel - INFO - [initialize] Complete. Best validation: 25.946207\n",
      "2025-10-26 13:26:31,151 - algos.engine.hparam_tuner - INFO -   Trial 7: loss = 25.946207\n",
      "[I 2025-10-26 13:26:31,164] Trial 7 finished with value: 25.94620704650879 and parameters: {'train_offset': '5B', 'window_len': 10, 'lambda_ic': 0.25, 'lambda_sync': 1, 'lambda_rankic': 0.2, 'temperature': 0.057511662994349455, 'sync_thr': 0.7787263031571959, 'lr_stage1': 0.0007060343315659148, 'lr_stage1_ft': 0.0023049333184987776, 'lr_stage2': 0.0006210981387697233, 'weight_decay': 0.0}. Best is trial 4 with value: 0.14732937514781952.\n",
      "2025-10-26 13:26:31,171 - algos.engine.hparam_tuner - INFO - \n",
      "Best model trial: 4\n",
      "2025-10-26 13:26:31,171 - algos.engine.hparam_tuner - INFO - Best model loss: 0.147329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model optimization complete!\n",
      "Best model path: logs\\Models\\UMIModel\\trial_4\\init.pt\n",
      "MLflow run ID: c86007776b3340e3a3fcf1a4579b6c86\n"
     ]
    }
   ],
   "source": [
    "# Run model hyperparameter optimization\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"ðŸ”¬ STAGE 2: MODEL HYPERPARAMETER OPTIMIZATION\")\n",
    "logger.info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "model_results = tuner.optimize_model(instrument_ids=instrument_ids)\n",
    "\n",
    "print(\"\\nâœ… Model optimization complete!\")\n",
    "print(f\"Best model path: {model_results['model_path']}\")\n",
    "print(f\"MLflow run ID: {model_results['mlflow_run_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a2a6c",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 3: Strategy Hyperparameter Optimization\n",
    "Optimize strategy hyperparameters using best model from Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b45dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 23:34:55,161 - __main__ - INFO - \n",
      "======================================================================\n",
      "2025-10-25 23:34:55,162 - __main__ - INFO - ðŸ“Š STAGE 3: STRATEGY HYPERPARAMETER OPTIMIZATION\n",
      "2025-10-25 23:34:55,162 - __main__ - INFO - ======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2d7847f7504b0bbd94bd2c1786b2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:34:55,555] Using an existing study with name 'TopKStrategy_UMIModel' instead of creating a new one.\n",
      "2025-10-25 23:34:55,577 - root - INFO - No previous old best trial of strategy hpo\n",
      "2025-10-25 23:34:55,577 - algos.engine.hparam_tuner - INFO - Running 2 strategy trials...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945baa0a105d43b6a62bbc32d45070a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-10-25 23:37:59,967] Trial 3 failed with parameters: {'top_k': 2, 'retrain_offset': '14D', 'warm_training_epochs': 19, 'optimizer_name': 'm2', 'optimizer_lookback': '30B'} because of the following error: IndexError('list index out of range').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\algos\\engine\\hparam_tuner.py\", line 420, in strategy_objective\n",
      "    metrics, time_series = self._backtest(\n",
      "                           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\algos\\engine\\hparam_tuner.py\", line 606, in _backtest\n",
      "    engine = node.get_engines()[0]\n",
      "             ~~~~~~~~~~~~~~~~~~^^^\n",
      "IndexError: list index out of range\n",
      "[W 2025-10-25 23:37:59,969] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m model_name = cfg[\u001b[33m'\u001b[39m\u001b[33mMODEL\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mPARAMS\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m strategy_results = \u001b[43mtuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… Strategy optimization complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy_results[\u001b[33m'\u001b[39m\u001b[33mhparams\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\algos\\engine\\hparam_tuner.py:459\u001b[39m, in \u001b[36mOptunaHparamsTuner.optimize_strategy\u001b[39m\u001b[34m(self, model_name)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strategy_params.get(\u001b[33m\"\u001b[39m\u001b[33mtune_hparams\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m n_trials > \u001b[32m0\u001b[39m:\n\u001b[32m    458\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m strategy trials...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    461\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mStrategy hyperparameter tuning disabled, using defaults\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\algos\\engine\\hparam_tuner.py:420\u001b[39m, in \u001b[36mOptunaHparamsTuner.optimize_strategy.<locals>.strategy_objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    417\u001b[39m mlflow.log_artifact(\u001b[38;5;28mstr\u001b[39m(trial_config_path))\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# Run backtest with these strategy parameters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m metrics, time_series = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_params_flat\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_params_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy_params_flat\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_params_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_params_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalid_start\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy_params_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalid_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[38;5;66;03m# Generate and log charts\u001b[39;00m\n\u001b[32m    428\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating performance charts trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\algos\\engine\\hparam_tuner.py:606\u001b[39m, in \u001b[36mOptunaHparamsTuner._backtest\u001b[39m\u001b[34m(self, model_params_flat, strategy_params_flat, start, end)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# Run backtest\u001b[39;00m\n\u001b[32m    604\u001b[39m node.run()\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m engine = \u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    607\u001b[39m venue = Venue(\u001b[38;5;28mself\u001b[39m.strategy_params[\u001b[33m\"\u001b[39m\u001b[33mvenue_name\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m# Calculate metrics using Nautilus built-in analyzer\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Run strategy hyperparameter optimization\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"ðŸ“Š STAGE 3: STRATEGY HYPERPARAMETER OPTIMIZATION\")\n",
    "logger.info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "model_name = cfg['MODEL']['PARAMS']['model_name']\n",
    "strategy_results = tuner.optimize_strategy(model_name=model_name)\n",
    "\n",
    "print(\"\\nâœ… Strategy optimization complete!\")\n",
    "print(f\"Best hyperparameters: {strategy_results['hparams']}\")\n",
    "print(f\"\\nBest metrics:\")\n",
    "for metric, value in strategy_results['metrics'].items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "print(f\"\\nMLflow run ID: {strategy_results['mlflow_run_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491401ff",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 4: Final Backtest\n",
    "Run final backtest on full period with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimization context\n",
    "run = mlflow.get_run(strategy_results[\"mlflow_run_id\"])\n",
    "optimization_id = run.data.tags.get(\"optimization_id\", \"\")\n",
    "\n",
    "# Define backtest period\n",
    "backtest_start = cfg[\"STRATEGY\"][\"PARAMS\"][\"backtest_start\"]\n",
    "backtest_end = cfg[\"STRATEGY\"][\"PARAMS\"][\"backtest_end\"]\n",
    "\n",
    "print(f\"Running final backtest: {backtest_start} to {backtest_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute final backtest\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"ðŸš€ STAGE 4: FINAL BACKTEST\")\n",
    "logger.info(\"=\"*70 + \"\\n\")\n",
    "\n",
    "final_metrics, final_time_series = tuner.run_final_backtest(\n",
    "    backtest_start=backtest_start,\n",
    "    backtest_end=backtest_end,\n",
    "    strategy_hpo_run_id=strategy_results[\"mlflow_run_id\"],\n",
    "    optimization_id=optimization_id\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Final backtest complete!\")\n",
    "print(\"\\nFinal Performance Metrics:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in sorted(final_metrics.items()):\n",
    "    print(f\"{metric:.<40} {value:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dccb9a",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 5: Performance Analysis\n",
    "Detailed analysis and visualization of backtest results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from MLflow\n",
    "client = MlflowClient(tracking_uri=\"file:logs/mlflow\")\n",
    "\n",
    "# Get the most recent backtest run\n",
    "exp = client.get_experiment_by_name(\"Backtests\")\n",
    "if not exp:\n",
    "    exp_id = client.create_experiment(\"Backtests\")\n",
    "else:\n",
    "    exp_id = exp.experiment_id\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[exp_id],\n",
    "    order_by=[\"start_time DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "backtest_run = runs[0]\n",
    "backtest_run_id = backtest_run.info.run_id\n",
    "\n",
    "print(f\"Loading backtest run: {backtest_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6cbe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load account and positions data\n",
    "acc_path = client.download_artifacts(run_id=backtest_run_id, path=\"account_report.csv\")\n",
    "pos_path = client.download_artifacts(run_id=backtest_run_id, path=\"positions_report.csv\")\n",
    "\n",
    "account_df = pd.read_csv(acc_path, index_col=0, parse_dates=True)\n",
    "positions_df = pd.read_csv(pos_path, index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"Account snapshots: {len(account_df)}\")\n",
    "print(f\"Position records: {len(positions_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448314cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate returns\n",
    "freq = cfg['STRATEGY']['PARAMS']['freq']\n",
    "freq_params = get_frequency_params(freq)\n",
    "\n",
    "# Extract portfolio values\n",
    "currency_code = cfg['STRATEGY']['PARAMS']['currency'].code\n",
    "portfolio_values = account_df[account_df['currency'] == currency_code]['total']\n",
    "portfolio_values = portfolio_values.resample(freq_params['resample_freq']).last().ffill()\n",
    "strategy_ret = portfolio_values.pct_change().fillna(0)\n",
    "\n",
    "# Load benchmark and risk-free data\n",
    "data_dict = loader.get_ohlcv_data(frequency=freq)\n",
    "benchmark_ticker = cfg['STRATEGY']['PARAMS']['benchmark_ticker']\n",
    "risk_free_ticker = cfg['STRATEGY']['PARAMS']['risk_free_ticker']\n",
    "benchmark_ret = data_dict[benchmark_ticker]['close'].pct_change()\n",
    "rf_ret = data_dict[risk_free_ticker]['close'].pct_change()\n",
    "\n",
    "# Align series\n",
    "strategy_ret, benchmark_ret, rf_ret = align_series(\n",
    "    strategy_ret, benchmark_ret, rf_ret, freq_params['resample_freq']\n",
    ")\n",
    "\n",
    "print(f\"Returns calculated for {len(strategy_ret)} periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d53b1",
   "metadata": {},
   "source": [
    "### 5.1: Balance Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_balance_breakdown(\n",
    "    account_df=account_df,\n",
    "    resample_freq=freq_params['resample_freq']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7932ff",
   "metadata": {},
   "source": [
    "### 5.2: Cumulative Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751366d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_cumulative_returns(\n",
    "    strategy_ret=strategy_ret,\n",
    "    benchmark_ret=benchmark_ret\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949c63f",
   "metadata": {},
   "source": [
    "### 5.3: Rolling Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = max(10, int(freq_params['periods_per_year'] / 12))\n",
    "fig = plot_rolling_sharpe(\n",
    "    strategy_ret=strategy_ret,\n",
    "    benchmark_ret=benchmark_ret,\n",
    "    rf_ret=rf_ret,\n",
    "    window=window,\n",
    "    annualization_factor=freq_params['annualization_factor']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722fd1f8",
   "metadata": {},
   "source": [
    "### 5.4: Drawdown Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526edea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_underwater(strategy_ret=strategy_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c1b31",
   "metadata": {},
   "source": [
    "### 5.5: Active Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba177e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_active_returns(\n",
    "    strategy_ret=strategy_ret,\n",
    "    benchmark_ret=benchmark_ret,\n",
    "    freq=freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fda249",
   "metadata": {},
   "source": [
    "### 5.6: Portfolio Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e71db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_portfolio_allocation(\n",
    "    positions_df=positions_df,\n",
    "    resample_freq=freq_params['resample_freq']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660082b",
   "metadata": {},
   "source": [
    "### 5.7: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "total_return = (1 + strategy_ret).prod() - 1\n",
    "annualized_return = (1 + total_return) ** (freq_params['periods_per_year'] / len(strategy_ret)) - 1\n",
    "annualized_vol = strategy_ret.std() * freq_params['annualization_factor']\n",
    "sharpe_ratio = (strategy_ret.mean() - rf_ret.mean()) / strategy_ret.std() * freq_params['annualization_factor']\n",
    "\n",
    "# Drawdown\n",
    "cumulative = (1 + strategy_ret).cumprod()\n",
    "running_max = cumulative.expanding().max()\n",
    "drawdown = (cumulative - running_max) / running_max\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "# Win rate\n",
    "winning_periods = (strategy_ret > 0).sum()\n",
    "win_rate = winning_periods / len(strategy_ret)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Return (%)',\n",
    "        'Annualized Return (%)',\n",
    "        'Annualized Volatility (%)',\n",
    "        'Sharpe Ratio',\n",
    "        'Max Drawdown (%)',\n",
    "        'Win Rate (%)',\n",
    "        'Number of Periods'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{total_return * 100:.2f}\",\n",
    "        f\"{annualized_return * 100:.2f}\",\n",
    "        f\"{annualized_vol * 100:.2f}\",\n",
    "        f\"{sharpe_ratio:.2f}\",\n",
    "        f\"{max_drawdown * 100:.2f}\",\n",
    "        f\"{win_rate * 100:.2f}\",\n",
    "        len(strategy_ret)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773ed98",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison Matrix: All Models Ã— Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HPO results matrix\n",
    "hpo_matrix = tuner.get_strategy_hpo_matrix(metric=\"total_pnl_pct\")\n",
    "print(\"\\nStrategy HPO Results Matrix (total_pnl_pct):\")\n",
    "print(hpo_matrix)\n",
    "\n",
    "# Generate final backtest results matrix\n",
    "backtest_matrix = tuner.get_final_backtest_matrix(metric=\"sharpe_ratio\")\n",
    "print(\"\\nFinal Backtest Results Matrix (sharpe_ratio):\")\n",
    "print(backtest_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1faa12",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline Complete âœ…\n",
    "\n",
    "**Next Steps:**\n",
    "- Review MLflow UI: `mlflow ui --backend-store-uri logs/mlflow`\n",
    "- Explore experiment tracking and compare runs\n",
    "- Adjust hyperparameters in `configs/config.yaml` and rerun\n",
    "- Export results for production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
