{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Research_mlflow.ipynb - Model Training & MLflow Visualization\n",
                "==============================================================\n",
                "This notebook demonstrates model lifecycle (initialize, predict, update) \n",
                "with MLflow tracking for training metrics visualization.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Configuration loaded from: configs\\config.yaml\n",
                        "Model: UMIModel\n",
                        "Data directory: data/\n",
                        "Frequency: 1D\n",
                        "Retrain offset: 30B\n",
                        "Prediction length: 1 bars\n",
                        "MLflow tracking URI: file:./mlruns\n",
                        "Active experiment: Model_Research\n",
                        "Test universe: ['AAPL', 'AMZN', 'GOOGL']\n",
                        "  AAPL: 128 bars\n",
                        "  AMZN: 128 bars\n",
                        "  GOOGL: 128 bars\n",
                        "\n",
                        "=== Model Initialization ===\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "c:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using default hyperparameters...\n",
                        "Training model from scratch...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025/08/21 01:28:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
                        "2025/08/21 01:29:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initial training complete. Best validation loss: inf\n",
                        "\n",
                        "=== Testing Prediction ===\n"
                    ]
                },
                {
                    "ename": "AssertionError",
                    "evalue": "tiker dataframe size mismatch",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 161\u001b[39m\n\u001b[32m    158\u001b[39m current_time = valid_end\n\u001b[32m    159\u001b[39m active_mask = torch.ones(\u001b[38;5;28mlen\u001b[39m(test_universe), dtype=torch.bool)\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactive_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredictions at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ticker, pred \u001b[38;5;129;01min\u001b[39;00m predictions.items():\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\models\\UMIModel\\UMIModel.py:270\u001b[39m, in \u001b[36mUMIModel.predict\u001b[39m\u001b[34m(self, data, current_time, active_mask)\u001b[39m\n\u001b[32m    268\u001b[39m days_range = \u001b[38;5;28mself\u001b[39m.market_calendar.schedule(start_date=current_time, end_date=current_time + \u001b[38;5;28mself\u001b[39m.pred_offset)\n\u001b[32m    269\u001b[39m timestamps = market_calendars.date_range(days_range, frequency=\u001b[38;5;28mself\u001b[39m.freq).normalize()\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m data_tensor, data_mask = \u001b[43mbuild_pred_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# assertion on the universe size\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.equal(data_mask, active_mask), \u001b[33m\"\u001b[39m\u001b[33mActive mask mismatch during prediction\u001b[39m\u001b[33m\"\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gianc\\Desktop\\PYTHON\\sapiens\\models\\utils.py:311\u001b[39m, in \u001b[36mbuild_pred_tensor\u001b[39m\u001b[34m(data, timestamps, feature_dim, device)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Fill in data for each instrument\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, ticker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(universe):\n\u001b[32m    310\u001b[39m     \u001b[38;5;66;03m# Reindex to common timestamps (automatically fills NaN for missing)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m data[ticker].shape == (T,F), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtiker dataframe size mismatch\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# Copy values into pre-allocated array\u001b[39;00m\n\u001b[32m    313\u001b[39m     tensor_array[:, i, :] = torch.tensor(data[ticker].values, dtype=torch.float32, device=device)\n",
                        "\u001b[31mAssertionError\u001b[39m: tiker dataframe size mismatch"
                    ]
                }
            ],
            "source": [
                "\"\"\"\n",
                "research_mlflow.ipynb - Model Training & MLflow Visualization\n",
                "==============================================================\n",
                "This notebook demonstrates model lifecycle (initialize, predict, update) \n",
                "with MLflow tracking for training metrics visualization.\n",
                "\"\"\"\n",
                "\n",
                "# %% [markdown]\n",
                "# # UMI Model Training with MLflow Tracking\n",
                "# This notebook tests the model's core functions with sample data and visualizes training metrics\n",
                "\n",
                "# %% Import dependencies\n",
                "import calendar\n",
                "import sys\n",
                "import os\n",
                "sys.path.append(os.path.abspath('..'))\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import mlflow\n",
                "import mlflow.pytorch\n",
                "from pathlib import Path\n",
                "import yaml\n",
                "import matplotlib.pyplot as plt\n",
                "from datetime import datetime, timedelta\n",
                "from pandas.tseries.frequencies import to_offset\n",
                "import pandas_market_calendars as market_calendars\n",
                "\n",
                "# Import model and utilities\n",
                "from models.UMIModel.UMIModel import UMIModel\n",
                "from algos.engine.data_loader import CsvBarLoader\n",
                "from models.utils import freq2pdoffset, freq2pdoffset\n",
                "\n",
                "# %% Load configuration\n",
                "config_path = Path(\"configs/config.yaml\")\n",
                "cfg = yaml.safe_load(config_path.read_text(encoding=\"utf-8\"))\n",
                "\n",
                "print(f\"Configuration loaded from: {config_path}\")\n",
                "print(f\"Model: {cfg['model_name']}\")\n",
                "print(f\"Data directory: {cfg['data_dir']}\")\n",
                "print(f\"Frequency: {cfg['freq']}\")\n",
                "print(f\"Retrain offset: {cfg['training']['retrain_offset']}\")\n",
                "print(f\"Prediction length: {cfg['pred_len']} bars\")\n",
                "\n",
                "# %% Setup MLflow\n",
                "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
                "mlflow.set_experiment(\"Model_Research\")\n",
                "\n",
                "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
                "print(f\"Active experiment: {mlflow.get_experiment_by_name('Model_Research').name}\")\n",
                "\n",
                "# %% Load sample data\n",
                "# Initialize data loader\n",
                "cols = ['Open', 'High', 'Low', 'Adj Close', 'Volume']\n",
                "loader = CsvBarLoader(cfg=cfg, columns_to_load=cols)\n",
                "\n",
                "# Select a subset of stocks for testing\n",
                "test_universe = loader.universe[:10]  # Use first 10 stocks for testing\n",
                "print(f\"Test universe: {test_universe}\")\n",
                "\n",
                "# Prepare sample data windows\n",
                "backtest_start = pd.Timestamp(cfg[\"backtest_start\"], tz=\"UTC\")\n",
                "train_offset = freq2pdoffset(cfg[\"train_offset\"])\n",
                "valid_offset = freq2pdoffset(cfg[\"valid_offset\"])\n",
                "train_end = backtest_start + train_offset\n",
                "valid_end = train_end\n",
                "\n",
                "# Create data dictionary for selected stocks\n",
                "sample_data = {}\n",
                "calendar = market_calendars.get_calendar(cfg[\"calendar\"])\n",
                "days_range = calendar.schedule(start_date=backtest_start, end_date=valid_end)\n",
                "timestamps = market_calendars.date_range(days_range, frequency=cfg[\"freq\"]).normalize()\n",
                "for ticker in test_universe:\n",
                "    if ticker in loader._frames:\n",
                "        df = loader._frames[ticker]\n",
                "        # Get data up to validation end for initial training\n",
                "        df.index = df.index.normalize()\n",
                "        sample_data[ticker] = df.reindex(timestamps).dropna()\n",
                "        print(f\"  {ticker}: {len(sample_data[ticker])} bars\")\n",
                "\n",
                "assert len(sample_data) > 0 \n",
                "# %% Initialize model with MLflow tracking\n",
                "print(\"\\n=== Model Initialization ===\")\n",
                "\n",
                "# Start MLflow run\n",
                "with mlflow.start_run(run_name=f\"research_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
                "    \n",
                "    # Model parameters\n",
                "    model_params = {\n",
                "        \"freq\": cfg[\"freq\"],\n",
                "        \"feature_dim\": cfg[\"feature_dim\"],\n",
                "        \"window_len\": cfg[\"window_len\"],\n",
                "        \"pred_len\": cfg[\"pred_len\"],\n",
                "        \"train_offset\": freq2pdoffset(cfg[\"train_offset\"]),\n",
                "        \"pred_offset\": freq2pdoffset(cfg[\"freq\"])*int(cfg[\"pred_len\"]),\n",
                "        \"train_end\": train_end,\n",
                "        \"valid_end\": valid_end,\n",
                "        \"n_epochs\": 1,  # Reduced for testing\n",
                "        \"batch_size\": cfg[\"training\"][\"batch_size\"],\n",
                "        \"patience\": cfg[\"training\"][\"patience\"],\n",
                "        \"pretrain_epochs\": cfg[\"training\"][\"pretrain_epochs\"],\n",
                "        \"training_mode\": cfg[\"training\"][\"training_mode\"],\n",
                "        \"close_idx\": cfg[\"training\"][\"target_idx\"],\n",
                "        \"warm_start\": cfg[\"training\"][\"warm_start\"],\n",
                "        \"warm_training_epochs\": cfg[\"training\"][\"warm_training_epochs\"],\n",
                "        \"save_backups\": False,\n",
                "        \"data_dir\": Path(cfg[\"data_dir\"]),\n",
                "        \"model_dir\": Path(\"logs/research_model\"),\n",
                "        \"calendar\": cfg[\"calendar\"],   #Equity market calendar\n",
                "    }\n",
                "    \n",
                "    # Use HP tuner like strategy does\n",
                "    from algos.engine.hparam_tuner import OptunaHparamsTuner, split_hparam_cfg\n",
                "    \n",
                "    defaults, search_space = split_hparam_cfg(cfg[\"hparams\"])\n",
                "    hp_values = {}\n",
                "    \n",
                "    # Test hyperparameter tuning workflow\n",
                "    if cfg[\"training\"].get(\"tune_hparams\", False):\n",
                "        print(\"Testing HPO workflow...\")\n",
                "        tuner = OptunaHparamsTuner(\n",
                "            model_name=cfg[\"model_name\"],\n",
                "            ModelClass=UMIModel,\n",
                "            start=backtest_start,\n",
                "            end=valid_end,\n",
                "            logs_dir=Path(\"logs/research_hpo\"),\n",
                "            data=sample_data,\n",
                "            model_params=model_params,\n",
                "            defaults=defaults,\n",
                "            search_space=search_space,\n",
                "            n_trials=4,  # Just 2 trials for testing\n",
                "            log=None,\n",
                "        )\n",
                "        best = tuner.optimize()\n",
                "        hp_values = {**defaults, **best[\"hparams\"]}\n",
                "        print(f\"HPO complete. Best hparams: {hp_values}\")\n",
                "    else:\n",
                "        print(\"Using default hyperparameters...\")\n",
                "        hp_values = defaults\n",
                "\n",
                "    # Create model instance\n",
                "    model = UMIModel(**model_params, **hp_values)\n",
                "    \n",
                "    # Initialize model (trains from scratch)\n",
                "    print(\"Training model from scratch...\")\n",
                "    best_val_loss = model.initialize(sample_data)\n",
                "    print(f\"Initial training complete. Best validation loss: {best_val_loss:.6f}\")\n",
                "    \n",
                "    # Log model info\n",
                "    mlflow.log_param(\"num_stocks\", len(test_universe))\n",
                "    mlflow.log_param(\"train_bars\", len(sample_data[test_universe[0]]))\n",
                "    \n",
                "    # %% Test prediction\n",
                "    print(\"\\n=== Testing Prediction ===\")\n",
                "    \n",
                "    # Prepare current data for prediction (use last window)\n",
                "    current_time = valid_end\n",
                "    active_mask = torch.ones(len(test_universe), dtype=torch.bool)\n",
                "    \n",
                "    predictions = model.predict(sample_data, current_time, active_mask)\n",
                "    \n",
                "    print(f\"Predictions at {current_time}:\")\n",
                "    for ticker, pred in predictions.items():\n",
                "        print(f\"  {ticker}: {pred:.6f}\")\n",
                "    \n",
                "    # Log predictions\n",
                "    for ticker, pred in predictions.items():\n",
                "        mlflow.log_metric(f\"pred_{ticker}\", pred)\n",
                "    \n",
                "    # %% Test walk-forward updates until retrain_offset\n",
                "    print(\"\\n=== Testing Walk-Forward Updates ===\")\n",
                "    print(\"This simulates the model's behavior during live trading:\")\n",
                "    print(\"1. Make predictions at each time step\")\n",
                "    print(\"2. Update model with warm-start training\")\n",
                "    print(\"3. Continue until retrain_offset is reached\")\n",
                "    print(\"4. Then re-initialize model from scratch\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    # Get update and retrain intervals from config\n",
                "    from pandas.tseries.frequencies import to_offset\n",
                "    update_freq = to_offset(cfg[\"freq\"]) * cfg[\"pred_len\"]  # Update after each prediction horizon\n",
                "    retrain_offset = to_offset(cfg[\"training\"][\"retrain_offset\"])\n",
                "    \n",
                "    print(f\"Update frequency: {update_freq}\")\n",
                "    print(f\"Retrain offset: {retrain_offset}\")\n",
                "    \n",
                "    # Track predictions over time\n",
                "    prediction_history = []\n",
                "    current_time = valid_end\n",
                "    last_retrain_time = valid_end\n",
                "    update_count = 0\n",
                "    \n",
                "    # Walk forward until retrain_offset is reached\n",
                "    while current_time - last_retrain_time < retrain_offset:\n",
                "        # Move forward by prediction horizon\n",
                "        current_time = current_time + update_freq\n",
                "        update_count += 1\n",
                "        \n",
                "        # Check if we have data for this time\n",
                "        data_available = all(\n",
                "            ticker in loader._frames and \n",
                "            len(loader._frames[ticker][loader._frames[ticker].index <= current_time]) >= cfg[\"window_len\"]\n",
                "            for ticker in test_universe\n",
                "        )\n",
                "        \n",
                "        if not data_available:\n",
                "            print(f\"Insufficient data at {current_time}, stopping walk-forward\")\n",
                "            break\n",
                "        \n",
                "        # Prepare data up to current time\n",
                "        current_data = {}\n",
                "        for ticker in test_universe:\n",
                "            if ticker in loader._frames:\n",
                "                df = loader._frames[ticker]\n",
                "                mask = (df.index >= backtest_start) & (df.index <= current_time)\n",
                "                current_data[ticker] = df[mask]\n",
                "        \n",
                "        # Make prediction\n",
                "        predictions = model.predict(current_data, current_time, active_mask)\n",
                "        \n",
                "        # Store predictions\n",
                "        pred_record = {\"timestamp\": current_time, \"update_num\": update_count}\n",
                "        pred_record.update({f\"pred_{ticker}\": pred for ticker, pred in predictions.items()})\n",
                "        prediction_history.append(pred_record)\n",
                "        \n",
                "        # Update model (warm-start training)\n",
                "        print(f\"Update {update_count}: {current_time.strftime('%Y-%m-%d')} - Updating model...\")\n",
                "        \n",
                "        # Update model's time windows for warm-start training\n",
                "        model.train_end = current_time\n",
                "        model.valid_end = current_time\n",
                "        \n",
                "        model.update(current_data, current_time, active_mask)\n",
                "        \n",
                "        # Calculate time since last retrain for tracking\n",
                "        time_since_retrain = (current_time - last_retrain_time).days\n",
                "        \n",
                "        # Log to MLflow\n",
                "        for ticker, pred in predictions.items():\n",
                "            mlflow.log_metric(f\"walkfwd_pred_{ticker}\", pred, step=update_count)\n",
                "        mlflow.log_metric(\"walkfwd_update\", update_count, step=update_count)\n",
                "        mlflow.log_metric(\"days_since_retrain\", time_since_retrain, step=update_count)\n",
                "        \n",
                "        # Print progress\n",
                "        if update_count % 5 == 0 or update_count == 1:\n",
                "            sample_pred = predictions[test_universe[0]]\n",
                "            print(f\"  Update {update_count}: Time={current_time.strftime('%Y-%m-%d')}, \"\n",
                "                  f\"Sample pred ({test_universe[0]})={sample_pred:.6f}\")\n",
                "    \n",
                "    print(f\"\\n✅ Completed {update_count} updates over {current_time - valid_end}\")\n",
                "    print(f\"Time since last retrain: {current_time - last_retrain_time}\")\n",
                "    \n",
                "    # Convert predictions to DataFrame for analysis\n",
                "    pred_df = pd.DataFrame(prediction_history)\n",
                "    pred_df.set_index('timestamp', inplace=True)\n",
                "    \n",
                "    # Visualize prediction evolution\n",
                "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
                "    \n",
                "    # Top plot: Prediction evolution\n",
                "    for ticker in test_universe[:3]:  # Plot first 3 tickers\n",
                "        pred_col = f\"pred_{ticker}\"\n",
                "        if pred_col in pred_df.columns:\n",
                "            ax1.plot(pred_df.index, pred_df[pred_col], marker='o', label=ticker, alpha=0.7)\n",
                "    \n",
                "    ax1.set_title(\"Prediction Evolution During Walk-Forward\")\n",
                "    ax1.set_xlabel(\"Time\")\n",
                "    ax1.set_ylabel(\"Predicted Return\")\n",
                "    ax1.legend()\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Bottom plot: Prediction variance over time (model degradation indicator)\n",
                "    pred_cols = [col for col in pred_df.columns if col.startswith('pred_')]\n",
                "    if len(pred_cols) > 0:\n",
                "        rolling_std = pred_df[pred_cols].rolling(window=min(5, len(pred_df))).std().mean(axis=1)\n",
                "        ax2.plot(pred_df.index, rolling_std, marker='s', color='red', alpha=0.7)\n",
                "        ax2.set_title(\"Model Stability (Rolling Std of Predictions)\")\n",
                "        ax2.set_xlabel(\"Time\")\n",
                "        ax2.set_ylabel(\"Prediction Volatility\")\n",
                "        ax2.grid(True, alpha=0.3)\n",
                "        \n",
                "        # Log degradation metric\n",
                "        if len(rolling_std) > 1:\n",
                "            degradation = (rolling_std.iloc[-1] - rolling_std.iloc[0]) / rolling_std.iloc[0] if rolling_std.iloc[0] != 0 else 0\n",
                "            mlflow.log_metric(\"model_degradation_pct\", degradation * 100)\n",
                "            ax2.text(0.02, 0.98, f\"Degradation: {degradation*100:.1f}%\", \n",
                "                    transform=ax2.transAxes, verticalalignment='top',\n",
                "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
                "    \n",
                "    plt.xticks(rotation=45)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # %% Test model re-initialization after retrain_offset\n",
                "    print(\"\\n=== Testing Model Re-initialization ===\")\n",
                "    print(f\"Re-initializing model at {current_time} (after {update_count} updates)\")\n",
                "    \n",
                "    # Prepare extended training data for re-initialization\n",
                "    retrain_data = {}\n",
                "    for ticker in test_universe:\n",
                "        if ticker in loader._frames:\n",
                "            df = loader._frames[ticker]\n",
                "            # Include more recent data for retraining\n",
                "            mask = (df.index >= backtest_start) & (df.index <= current_time)\n",
                "            retrain_data[ticker] = df[mask]\n",
                "    \n",
                "    # Update model's train/valid end dates for re-initialization\n",
                "    model.train_end = current_time - pd.DateOffset(days=30)  # Keep some data for validation\n",
                "    model.valid_end = current_time\n",
                "    \n",
                "    # Re-initialize model (full retraining from scratch)\n",
                "    print(\"Re-training model from scratch with updated data...\")\n",
                "    model._is_initialized = False  # Reset initialization flag\n",
                "    new_val_loss = model.initialize(retrain_data)\n",
                "    print(f\"Re-initialization complete. New validation loss: {new_val_loss:.6f}\")\n",
                "    \n",
                "    # Log re-initialization metrics\n",
                "    mlflow.log_metric(\"retrain_val_loss\", new_val_loss)\n",
                "    mlflow.log_metric(\"retrain_after_updates\", update_count)\n",
                "    \n",
                "    # Make predictions with re-initialized model\n",
                "    retrain_predictions = model.predict(retrain_data, current_time, active_mask)\n",
                "    \n",
                "    print(f\"\\nPredictions after re-initialization:\")\n",
                "    for ticker in test_universe[:5]:\n",
                "        if ticker in retrain_predictions:\n",
                "            print(f\"  {ticker}: {retrain_predictions[ticker]:.6f}\")\n",
                "    \n",
                "    # %% Compare predictions before and after re-initialization\n",
                "    print(\"\\n=== Comparing Predictions: Before vs After Re-initialization ===\")\n",
                "    \n",
                "    if len(prediction_history) > 0:\n",
                "        # Get last predictions before re-initialization\n",
                "        last_update_preds = prediction_history[-1]\n",
                "        \n",
                "        comparison_data = []\n",
                "        for ticker in test_universe:\n",
                "            before = last_update_preds.get(f\"pred_{ticker}\", 0)\n",
                "            after = retrain_predictions.get(ticker, 0)\n",
                "            change = after - before\n",
                "            change_pct = (change / abs(before) * 100) if before != 0 else 0\n",
                "            \n",
                "            comparison_data.append({\n",
                "                \"Ticker\": ticker,\n",
                "                \"Before Retrain\": f\"{before:.6f}\",\n",
                "                \"After Retrain\": f\"{after:.6f}\",\n",
                "                \"Change\": f\"{change:.6f}\",\n",
                "                \"Change %\": f\"{change_pct:.2f}%\"\n",
                "            })\n",
                "        \n",
                "        comparison_df = pd.DataFrame(comparison_data)\n",
                "        print(comparison_df.to_string(index=False))\n",
                "        \n",
                "        # Visualize the comparison\n",
                "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
                "        \n",
                "        # Plot 1: Before vs After\n",
                "        tickers = [d[\"Ticker\"] for d in comparison_data]\n",
                "        before_vals = [float(d[\"Before Retrain\"]) for d in comparison_data]\n",
                "        after_vals = [float(d[\"After Retrain\"]) for d in comparison_data]\n",
                "        \n",
                "        x = np.arange(len(tickers))\n",
                "        width = 0.35\n",
                "        \n",
                "        ax1.bar(x - width/2, before_vals, width, label='Before Retrain', alpha=0.7)\n",
                "        ax1.bar(x + width/2, after_vals, width, label='After Retrain', alpha=0.7)\n",
                "        ax1.set_xlabel('Ticker')\n",
                "        ax1.set_ylabel('Predicted Return')\n",
                "        ax1.set_title('Predictions: Before vs After Re-initialization')\n",
                "        ax1.set_xticks(x)\n",
                "        ax1.set_xticklabels(tickers, rotation=45)\n",
                "        ax1.legend()\n",
                "        ax1.grid(True, alpha=0.3)\n",
                "        \n",
                "        # Plot 2: Change distribution\n",
                "        changes = [float(d[\"Change\"]) for d in comparison_data]\n",
                "        ax2.bar(tickers, changes, alpha=0.7, color=['g' if c > 0 else 'r' for c in changes])\n",
                "        ax2.set_xlabel('Ticker')\n",
                "        ax2.set_ylabel('Change in Prediction')\n",
                "        ax2.set_title('Change After Re-initialization')\n",
                "        ax2.set_xticklabels(tickers, rotation=45)\n",
                "        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
                "        ax2.grid(True, alpha=0.3)\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "        \n",
                "        # Log comparison metrics\n",
                "        avg_change = np.mean(np.abs(changes))\n",
                "        mlflow.log_metric(\"avg_prediction_change_after_retrain\", avg_change)\n",
                "        print(f\"\\nAverage absolute change in predictions: {avg_change:.6f}\")\n",
                "\n",
                "# %% Visualize MLflow metrics\n",
                "print(\"\\n=== MLflow Metrics Visualization ===\")\n",
                "\n",
                "# Get the last run\n",
                "client = mlflow.tracking.MlflowClient()\n",
                "experiment = client.get_experiment_by_name(\"Model_Research\")\n",
                "runs = client.search_runs(experiment_ids=[experiment.experiment_id], max_results=1)\n",
                "\n",
                "if runs:\n",
                "    run_id = runs[0].info.run_id\n",
                "    print(f\"Visualizing metrics from run: {run_id}\")\n",
                "    \n",
                "    # Get metrics history\n",
                "    metric_names = [\"train_loss_stock\", \"train_loss_market\", \"train_loss_pred\", \"val_loss\"]\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
                "    axes = axes.flatten()\n",
                "    \n",
                "    for i, metric_name in enumerate(metric_names):\n",
                "        metric_history = client.get_metric_history(run_id, metric_name)\n",
                "        if metric_history:\n",
                "            steps = [m.step for m in metric_history]\n",
                "            values = [m.value for m in metric_history]\n",
                "            axes[i].plot(steps, values, marker='o')\n",
                "            axes[i].set_title(metric_name)\n",
                "            axes[i].set_xlabel(\"Epoch\")\n",
                "            axes[i].set_ylabel(\"Loss\")\n",
                "            axes[i].grid(True)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Print final metrics\n",
                "    print(\"\\n=== Final Metrics ===\")\n",
                "    for key, value in runs[0].data.metrics.items():\n",
                "        print(f\"{key}: {value:.6f}\")\n",
                "\n",
                "# %% Compare multiple training runs (if available)\n",
                "print(\"\\n=== Training Runs Comparison ===\")\n",
                "\n",
                "# Get all runs from the experiment\n",
                "all_runs = client.search_runs(experiment_ids=[experiment.experiment_id])\n",
                "\n",
                "if len(all_runs) > 1:\n",
                "    # Create comparison dataframe\n",
                "    comparison_data = []\n",
                "    for run in all_runs:\n",
                "        run_data = {\n",
                "            \"run_id\": run.info.run_id[:8],\n",
                "            \"start_time\": pd.Timestamp(run.info.start_time, unit='ms'),\n",
                "            \"duration_min\": (run.info.end_time - run.info.start_time) / 60000,\n",
                "        }\n",
                "        # Add metrics\n",
                "        for key, value in run.data.metrics.items():\n",
                "            if \"best\" in key or \"final\" in key:\n",
                "                run_data[key] = value\n",
                "        comparison_data.append(run_data)\n",
                "    \n",
                "    comparison_df = pd.DataFrame(comparison_data)\n",
                "    print(comparison_df.to_string())\n",
                "else:\n",
                "    print(\"Only one run available. Run the notebook multiple times to compare runs.\")\n",
                "\n",
                "# %% Summary statistics for walk-forward simulation\n",
                "print(\"\\n=== Walk-Forward Simulation Summary ===\")\n",
                "print(f\"Total updates performed: {update_count}\")\n",
                "print(f\"Time period covered: {valid_end} to {current_time}\")\n",
                "print(f\"Days between updates: {update_freq.days if hasattr(update_freq, 'days') else 'varies'}\")\n",
                "print(f\"Retrain offset: {retrain_offset}\")\n",
                "\n",
                "# Calculate prediction stability\n",
                "if len(prediction_history) > 1:\n",
                "    pred_cols = [col for col in pred_df.columns if col.startswith('pred_')]\n",
                "    for col in pred_cols:\n",
                "        ticker = col.replace('pred_', '')\n",
                "        values = pred_df[col].values\n",
                "        stability = np.std(values)\n",
                "        drift = values[-1] - values[0] if len(values) > 0 else 0\n",
                "        print(f\"\\n{ticker} predictions:\")\n",
                "        print(f\"  Stability (std): {stability:.6f}\")\n",
                "        print(f\"  Drift (last-first): {drift:.6f}\")\n",
                "        print(f\"  Mean: {np.mean(values):.6f}\")\n",
                "\n",
                "# Log summary metrics\n",
                "mlflow.log_metrics({\n",
                "    \"total_updates\": update_count,\n",
                "    \"prediction_stability_mean\": np.mean([np.std(pred_df[col].values) for col in pred_cols]),\n",
                "    \"final_retrain_loss\": new_val_loss,\n",
                "})\n",
                "\n",
                "# %% MLflow UI Instructions\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"To view detailed metrics in MLflow UI:\")\n",
                "print(\"1. Open terminal in the notebook directory\")\n",
                "print(\"2. Run: mlflow ui --backend-store-uri file:./mlruns\")\n",
                "print(\"3. Open browser at http://localhost:5000\")\n",
                "print(\"4. Compare runs, view metrics, and analyze experiments\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# %% Test hyperparameter impact\n",
                "print(\"\\n=== Testing Hyperparameter Impact ===\")\n",
                "\n",
                "# Test different learning rates\n",
                "test_hparams = [\n",
                "    {\"lr_stage1\": 0.0001, \"lr_stage2\": 0.00001},\n",
                "    {\"lr_stage1\": 0.001, \"lr_stage2\": 0.0001},\n",
                "    {\"lr_stage1\": 0.005, \"lr_stage2\": 0.001},\n",
                "]\n",
                "\n",
                "results = []\n",
                "for hp_set in test_hparams:\n",
                "    with mlflow.start_run(run_name=f\"hp_test_lr{hp_set['lr_stage1']}\"):\n",
                "        # Update hyperparameters\n",
                "        test_hp = hp_values.copy()\n",
                "        test_hp.update(hp_set)\n",
                "        \n",
                "        # Create and train model\n",
                "        test_model = UMIModel(**model_params, **test_hp)\n",
                "        val_loss = test_model.initialize(sample_data)\n",
                "        \n",
                "        # Log results\n",
                "        mlflow.log_params(hp_set)\n",
                "        mlflow.log_metric(\"final_val_loss\", val_loss)\n",
                "        \n",
                "        results.append({\n",
                "            **hp_set,\n",
                "            \"val_loss\": val_loss\n",
                "        })\n",
                "        \n",
                "        print(f\"LR={hp_set['lr_stage1']}: Val Loss={val_loss:.6f}\")\n",
                "\n",
                "# Display results\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\n=== Hyperparameter Test Results ===\")\n",
                "print(results_df.to_string())\n",
                "\n",
                "print(\"\\n✅ Model testing complete! Check MLflow UI for detailed metrics.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
